{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install livelossplot\n# !pip install git+git://github.com/stared/livelossplot.git","metadata":{"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport mne\nimport math\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.style as plt_style\nimport seaborn as sns\nimport tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport pytorch_lightning as pl\n\nimport umap\n\nplt_style.use('ggplot')\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":3.29809,"end_time":"2021-03-18T04:13:57.953711","exception":false,"start_time":"2021-03-18T04:13:54.655621","status":"completed"},"tags":[],"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_lightning.utilities import rank_zero_only\nfrom pytorch_lightning.loggers.base import rank_zero_experiment\nfrom livelossplot import PlotLosses\n\nclass LiveLossLogger(pl.loggers.LightningLoggerBase):\n    \n    def __init__(self):\n        super().__init__()\n        self.liveloss = PlotLosses()\n\n    @property\n    def name(self):\n        return 'MyLogger'\n\n    @property\n    @rank_zero_experiment\n    def experiment(self):\n        # Return the experiment object associated with this logger.\n        pass\n\n    @property\n    def version(self):\n        # Return the experiment version, int or str.\n        return '0.1'\n\n    @rank_zero_only\n    def log_hyperparams(self, params):\n        # params is an argparse.Namespace\n        # your code to record hyperparameters goes here\n        pass\n\n    @rank_zero_only\n    def log_metrics(self, metrics, step):\n        # metrics is a dictionary of metric names and values\n        # your code to record metrics goes here\n        self.liveloss.update(metrics)\n        self.liveloss.send()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Flags","metadata":{}},{"cell_type":"code","source":"RUN_UMAP = False\nRUN_RNN = False\nRUN_VAE = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tasks\n\n### Questions\n1) How long should each epoch be?\n2) Raw data + LFADS vs PSA + LFADS vs STFT + ConvVAE?\n\n### Todo\n* Exploration: Q1     - Run various epoch lengths of raw data through convolutional autoencoder and UMAP embeddings\n* Exploration: Q1, Q2 - Run various epoch lengths of STFT through convolutional autoencoder and UMAP embeddings\n* Exploration: Q1     - Run various epoch lengths of raw data through convolutional LSTM and UMAP embeddings\n\n* Build LFADS network\n* Q2: Run raw data through LFADS network\n* Q2: Run PSAs through LFADS network\n\n### In Progress\n* Exploration: Q1, Q2 - Run various epoch lengths of PSA of individual epochs through convolutional LSTM and UMAP embeddings\n* (Sayan) Exploration: STFT each epoch and run through UMAP\n\n### Done\n* Visualize raw data\n* Epoch data\n* (Sayan) Find alternative to Kaggle\n* (Nizar) Exploration: power spectral analysis of each epoch\n* (Nizar) Exploration: Run PSA through UMAP","metadata":{"papermill":{"duration":0.006554,"end_time":"2021-03-18T04:13:57.967609","exception":false,"start_time":"2021-03-18T04:13:57.961055","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Dataset Preprocessing","metadata":{}},{"cell_type":"code","source":"data_preproc = \nfourier = np.fft.fft(data_preproc)\nfreq = np.fft.fftfreq(data_preproc.shape[0], d=1.0/sfreq)\nif one_sided:\n        return freq[(0 < freq)], power_spec[(0 < freq)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mne.set_log_level(\"WARNING\")\n\ndef ch_keep(edf_files):\n    ch_occurrences = {}\n    for pid, data_dict in edf_files.items():\n        edf = mne.io.read_raw_edf(data_dict[\"data\"][0])\n\n        for ch in edf.info['ch_names']:\n            if ch not in ch_occurrences:\n                ch_occurrences[ch] = 1\n            else:\n                ch_occurrences[ch] += 1\n\n    ch_occurrences_list = []\n    for ch, occurrences in ch_occurrences.items():\n        ch_occurrences_list.append({\"Channel\": ch, \"Occurrences\": occurrences})\n    ch_df = pd.DataFrame(ch_occurrences_list)\n    max_occ = ch_df[\"Occurrences\"].max()\n    ch_include_df = ch_df[ch_df['Occurrences']==max_occ]\n    ch_include_df.reset_index(drop=True)\n    return list(ch_include_df[\"Channel\"])\n\ndef create_epochs(eeg_edf, channels_keep, preload=False, duration=5.0, overlap=2.5, bleed=0.2):\n    raw = mne.io.read_raw_edf(eeg_edf)\n    sfreq = raw.info['sfreq']\n    powerline_freqs = np.arange(60, sfreq/2, 60)\n    raw_notch = raw.load_data().copy().notch_filter(freqs=powerline_freqs) # remove powerline noise\n    \n    events = mne.make_fixed_length_events(raw_notch, start=0, stop=None, duration=duration+bleed, overlap=overlap+bleed)\n    epochs = mne.Epochs(raw_notch, events, tmin=-1.0*bleed, tmax=duration, baseline=(None, 0.0), preload=preload)\n    \n    excluded = ['PHOTIC-REF','IBI','BURSTS','SUPPR']\n    ch_drop = [x for x in raw.info[\"ch_names\"] if x not in channels_keep or x in excluded]\n    epochs.drop_channels(ch_drop)\n    return epochs\n\ndef get_psds(epochs):\n    psds,freqs=mne.time_frequency.psd_array_welch(epochs.get_data(), sfreq=epochs.info['sfreq'], average='mean')\n    return psds,freqs\n\ndef get_edfs_by_patient(root_dir, n=25):\n    edfs_by_patients_dict = {}\n    \n    pid_ind = -3\n    ann_ind = -6\n    num=0\n    for root,d_names,f_names in os.walk(root_dir):\n        for f in f_names:\n            file_path = os.path.join(root, f).split('/')\n            if os.path.splitext(f)[-1] == \".edf\":\n                if file_path[pid_ind] not in edfs_by_patients_dict:\n                    if num>n:\n                        return edfs_by_patients_dict\n                    edfs_by_patients_dict[file_path[pid_ind]] = {\"data\": [os.path.join(root, f)],\"annotations\": file_path[ann_ind]}\n                    num+=1\n                #else:\n                #    edfs_by_patients_dict[file_path[pid_ind]][\"data\"].append(os.path.join(root, f))    \n                    \n    return edfs_by_patients_dict\n\n\ndef files_df(edfs):\n    label = lambda x: 1 if x=='epilepsy' else 0\n    # file | ep/non-ep | pid\n    dataset_list = []\n    for patient in edfs:\n        for f in edfs[patient][\"data\"]:\n            dataset_list.append({'filename': f, 'patient_id': patient, 'label': label(edfs[patient]['annotations'])})\n    return pd.DataFrame(dataset_list)\n\ndef epoch_df(data_df, channels_keep):\n    os.makedirs(\"/kaggle/working/data/\", exist_ok=True)\n    epochs_list=[]\n    for i in tqdm.tqdm(range(len(data_df))):\n        entry = data_df.iloc[i]\n        filename, patient, label = entry['filename'], entry['patient_id'], entry['label']\n        epochs = create_epochs(filename, channels_keep, preload=True)\n        psds, _ = get_psds(epochs)\n        # print(psds.shape)\n        for idx, psd in enumerate(psds):\n            epoch_filename = \"/kaggle/working/data/\"+filename[9:-4].replace(\"/\", \"_\") + f\"_{idx}.npy\"\n            np.save(epoch_filename, psd)\n            epochs_list.append({'filename': epoch_filename, 'patient_id': patient, 'label': label})\n    epoch_dataframe = pd.DataFrame(epochs_list)\n    epoch_dataframe.to_csv(\"/kaggle/working/data.csv\")\n    return epoch_dataframe\n\ndef preprocessing_pipeline(n=25):\n    root_dir_ep = '../input/tuh-eeg-epilepsy-v100/edf/epilepsy/'\n    root_dir_no_ep = '../input/tuh-eeg-epilepsy-v100/edf/no_epilepsy/'\n    patient_edfs = get_edfs_by_patient(root_dir_ep, n)\n    patient_edfs.update(get_edfs_by_patient(root_dir_no_ep, n))\n    channels_keep = ch_keep(patient_edfs)\n    files = files_df(patient_edfs)\n    epochs = epoch_df(files, channels_keep)\n    return epochs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patient_edfs = get_edfs_by_patient('../input/tuh-eeg-epilepsy-v100/edf/epilepsy/01_tcp_ar/003/')\npatient_edfs.update(get_edfs_by_patient('../input/tuh-eeg-epilepsy-v100/edf/no_epilepsy/01_tcp_ar/027/'))\npatient_edfs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = preprocessing_pipeline(25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for x in tqdm.tqdm(range(len(epochs))):\n    test = np.load(epochs.iloc[x]['filename'])\nprint(\"All good.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Save global mean and sd for normalizing","metadata":{}},{"cell_type":"code","source":"N=1000\nrnd_idxs = np.random.choice(len(epochs), N, replace=False)\nsample = np.stack([np.load(epochs.iloc[x]['filename']) for x in rnd_idxs]);print(sample.shape)\nmean, sd = np.mean(sample, axis=0), np.std(sample, axis=0)\ntorch.save(mean, '/kaggle/working/means.pt')\ntorch.save(sd, '/kaggle/working/stds.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration: Basics","metadata":{}},{"cell_type":"code","source":"# with open(\"/kaggle/input/tuh-eeg-epilepsy-v100/edf/no_epilepsy/03_tcp_ar_a/076/00007671/s002_2011_02_03/00007671_s002.txt\") as example_eeg_descr_file:\n#     for line in example_eeg_descr_file.readlines():\n#         if line != \"\\n\":\n#             print(line)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_eeg = mne.io.read_raw_edf(\"../input/tuh-eeg-epilepsy-v100/edf/epilepsy/01_tcp_ar/003/00000355/s003_2013_01_04/00000355_s003_t000.edf\")\nexample_eeg.info","metadata":{"papermill":{"duration":0.073993,"end_time":"2021-03-18T04:13:58.128045","exception":false,"start_time":"2021-03-18T04:13:58.054052","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_CHANNELS = len(example_eeg.info.ch_names)\n\nsfreq = example_eeg.info['sfreq']\n\nt_start = 0\nt_end = len(example_eeg) / sfreq\n\n# Extract data from the first 5 channels, from 1 s to 3 s.\n\n# data, times = example_eeg[:NUM_CHANNELS,:]\ndata, times = example_eeg[:NUM_CHANNELS, int(sfreq * t_start):int(sfreq * t_end)]\n\nNUM_COLS = 3\nNUM_ROWS = math.ceil(NUM_CHANNELS / NUM_COLS)\n\nfig, axs = plt.subplots(NUM_ROWS, NUM_COLS, figsize=(30, 10), sharex=True)\naxs.shape\n\nchart_row = 0\nchart_col = 0\nfor ch_num, ch_name in enumerate(example_eeg.info.ch_names[:NUM_CHANNELS]):\n    axs[chart_row, chart_col].plot(times, data[ch_num])\n    axs[chart_row, chart_col].title.set_text(ch_name)\n    \n    chart_row += 1\n    if chart_row >= NUM_ROWS:\n        chart_row = 0\n        chart_col += 1\n\n# plt.xlabel('Seconds')\n# plt.ylabel('$\\mu V$')\n\n\n# plt.show()","metadata":{"papermill":{"duration":5.915534,"end_time":"2021-03-18T04:14:04.052326","exception":false,"start_time":"2021-03-18T04:13:58.136792","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration: UMAP","metadata":{}},{"cell_type":"code","source":"def get_spectogram(epochs):\n    freqs = np.linspace(1.0, epochs.info['sfreq'] / 2.0, epochs.times.shape[0])\n    n_cycles = 2.0 # max(freqs / 2.0, 2.0)\n    time_bandwidth = 3.0\n#     return mne.time_frequency.tfr_morlet(epochs, freqs=freqs,\n#                        n_cycles=n_cycles, return_itc=False, n_jobs=4, average=False)\n\n    return mne.time_frequency.tfr_multitaper(epochs, freqs=freqs, n_cycles=n_cycles,\n                       time_bandwidth=time_bandwidth, return_itc=False, average=False)\n\ndef UMAP(psds_of_patients_dict):\n    \"\"\" run non epilepsy epoch(s) and epilepsy epoch(s) through UMAP\n    \"\"\"\n    embeddings = {}\n    for p_id, psds in psds_of_patients_dict.items():\n        psds_of_patients_dict[p_id] = np.reshape(psds, (psds.shape[0], psds.shape[1]*psds.shape[2]))\n    \n    reducer = umap.UMAP()\n    \n    for p_id, psds in psds_of_patients_dict.items():\n        embedding = reducer.fit_transform(psds)\n        embeddings[p_id] = embedding\n\n    return embeddings\n\ndef plot_umap(embeddings_of_patients_dict, legend=False):\n    legend_labels = []\n    \n    for p_id, embedding in embeddings_of_patients_dict.items():\n        plt.scatter(embedding[:,0], embedding[:,1])\n        legend_labels.append(p_id)\n    \n    if legend:\n        plt.legend(legend_labels)\n        \ndef UMAP_example(edf_non_epilepsy, edf_epilepsy):\n    epochs_no_ep = create_epochs(edf_non_epilepsy)\n    epochs_ep = create_epochs(edf_epilepsy)    \n    psd_no_ep,_ = get_psds(epochs_no_ep)\n    psd_ep,_ = get_psds(epochs_ep)\n    return UMAP(psd_no_ep,psd_ep)","metadata":{"papermill":{"duration":21.69052,"end_time":"2021-03-18T04:14:25.767601","exception":false,"start_time":"2021-03-18T04:14:04.077081","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploring UMAP of EEG Power Spectra\n### Comparing non-epileptic patients only and epileptic patients only","metadata":{}},{"cell_type":"code","source":"if RUN_UMAP:\n    mne.set_log_level(verbose=\"WARNING\")\n\n    no_ep_files = get_edfs_by_patient(\"../input/tuh-eeg-epilepsy-v100/edf/no_epilepsy/01_tcp_ar/\")\n    ep_files = get_edfs_by_patient(\"../input/tuh-eeg-epilepsy-v100/edf/epilepsy/01_tcp_ar\")\n\n    no_ep_psds_by_patient = {}\n    ep_psds_by_patient = {}\n\n    # # print(\"---------------- EPOCHS: ----------------\")\n    # # print(epochs_no_ep.info)\n    # # print(\"No epilepsy:\", epochs_no_ep.get_data().shape, \"Epilepsy:\", epochs_ep.get_data().shape)\n    # # epochs_ep.plot_image()\n\n    for p_id, data_dict in no_ep_files.items():\n        epochs = create_epochs(data_dict[\"data\"])\n        psd, _ = get_psds(epochs)\n        no_ep_psds_by_patient[p_id] = psd\n\n    for p_id, data_dict in ep_files.items():\n        epochs = create_epochs(data_dict[\"data\"])\n        psd, _ = get_psds(epochs)\n        ep_psds_by_patient[p_id] = psd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RUN_UMAP:\n    no_ep_embeddings = UMAP(no_ep_psds_by_patient)\n    ep_embeddings = UMAP(ep_psds_by_patient)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RUN_UMAP:\n    plt.figure(figsize=(20,5))\n\n    plt.subplot(1, 2, 1)\n    plot_umap(no_ep_embeddings)\n    plt.title(\"PSD UMAP Embeddings from Non-Epileptic Patients\")\n\n    plt.subplot(1, 2, 2)\n    plot_umap(ep_embeddings)\n    plt.title(\"PSD UMAP Embeddings from Epileptic Patients\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comparing non-epileptic patients vs epileptic patients","metadata":{}},{"cell_type":"code","source":"if RUN_UMAP:\n    no_ep_embeddings_arr = np.concatenate(list(no_ep_embeddings.values()), axis=0)\n    ep_embeddings_arr = np.concatenate(list(ep_embeddings.values()), axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RUN_UMAP:\n    plt.figure(figsize=(20,5))\n\n    plt.subplot(1, 3, 1)\n    plt.scatter(no_ep_embeddings_arr[:,0], no_ep_embeddings_arr[:,1])\n    plt.scatter(ep_embeddings_arr[:,0], ep_embeddings_arr[:,1])\n    plt.legend([\"no ep\", \"ep\"])\n\n    plt.subplot(1, 3, 2)\n    plt.scatter(no_ep_embeddings_arr[:,0], no_ep_embeddings_arr[:,1])\n    plt.legend([\"no ep\"])\n\n    plt.subplot(1, 3, 3)\n    plt.scatter(ep_embeddings_arr[:,0], ep_embeddings_arr[:,1])\n    plt.legend([\"ep\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploring UMAP of EEG Spectograms (TBC)\n### Comparing non-epileptic patients only and epileptic patients only","metadata":{}},{"cell_type":"code","source":"# spectogram = get_spectogram(epochs_no_ep)\n# print(spectogram)\n# print(spectogram.info)\n# print(spectogram.data.shape)\n# print(spectogram.plot([0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic RNN Autoencoder","metadata":{}},{"cell_type":"code","source":"# TODO: NORMALIZE\ndef cross_correlation(x, y, num_channels):\n    # Note that PyTorch uses cross-correlation for their convolution operator instead of actual convolution\n    \n    corr = F.conv1d(x, y, groups=num_channels)\n        \n    return corr\n\n# cross_corr = cross_correlation(x, out, self.num_ch)\n# auto_corr = cross_correlation(x, x, self.num_ch)\n# loss = F.mse_loss(cross_corr, auto_corr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass MNEDataset(Dataset):\n    \n    def __init__(self, filename, drop_channels=[]):\n        self.epochs = create_epochs(filename, duration=5.0, overlap=4.5, bleed=0.2, preload=True)\n        \n        if len(drop_channels) > 0:\n            self.epochs.drop_channels(drop_channels)\n        \n        self.num_ch = self.epochs.info['nchan']\n        self.window_len = self.epochs[0].get_data().shape[2]\n        self.sfreq = self.epochs.info['sfreq']\n        self.ch_names = self.epochs.info['ch_names']\n        \n    def __len__(self):\n        return len(self.epochs)\n    \n    def __getitem__(self, index):\n        return torch.tensor(self.epochs[index].get_data()[0].T).float()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RNNAutoEncoder(pl.LightningModule):\n    def __init__(self, num_ch):\n        super().__init__()\n        self.num_ch = num_ch\n        \n        self.encoder = nn.GRU(input_size=num_ch,\n                              hidden_size=num_ch,\n                              num_layers=1,\n                              batch_first=True)\n        \n        self.decoder = nn.GRU(input_size=num_ch,\n                              hidden_size=num_ch,\n                              num_layers=1,\n                              batch_first=True)\n        \n        self.lr = 1.0e-3\n    \n    def forward(self, x):\n        encoder_out, encoder_hidden = self.encoder(x)\n        decoder_out, decoder_hidden = self.decoder(encoder_out, encoder_hidden)\n        \n        return encoder_out, encoder_hidden, decoder_out, decoder_hidden\n    \n    def calc_loss(self, x, out):\n        similarity_func = nn.CosineSimilarity(dim=1, eps=1.0e-10)\n        mse = nn.MSELoss()\n        \n        similarity = similarity_func(x, out)\n        \n        return mse(similarity, torch.ones(similarity.shape, device=self.device))\n    \n    def training_step(self, batch, batch_idx):\n        # training_step defined the train loop.\n        # It is independent of forward\n        x = batch\n        encoder_out, encoder_hidden, out, hidden = self(x)\n        \n        loss = self.calc_loss(x, out)\n\n        # Logging to TensorBoard by default\n        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x = batch\n        encoder_out, encoder_hidden, out, hidden = self(x)\n        \n        loss = self.calc_loss(x, out)\n\n        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RUN_RNN:\n    dataset_path = \"../input/tuh-eeg-epilepsy-v100/edf/epilepsy/01_tcp_ar/003/00000355/s003_2013_01_04/00000355_s003_t000.edf\"\n    dataset = MNEDataset(dataset_path,\n                         drop_channels=['PHOTIC-REF',\n                                        'IBI',\n                                        'BURSTS',\n                                        'SUPPR']\n                        )\n\n    train_len = int(len(dataset)*0.95)\n    val_len = len(dataset)-train_len\n\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, val_len])\n\n    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n    val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LOAD = True\ncheckpoint_path = \"../input/seizure-risk-assessment-results/sess_and_model.ckpt\"\n\nif RUN_RNN:\n    model = RNNAutoEncoder.load_from_checkpoint(checkpoint_path=checkpoint_path, num_ch=dataset.num_ch) if LOAD else RNNAutoEncoder(dataset.num_ch)\n    trainer = pl.Trainer(gpus=1,\n                         #logger=LiveLossLogger(),\n                         max_epochs=100,\n                         callbacks=[\n                             pl.callbacks.early_stopping.EarlyStopping(\n                                 monitor='train_loss',\n                                 mode='min',\n                                 patience=5\n                             )\n                         ],\n                        auto_lr_find=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RUN_RNN:\n    trainer.fit(model, train_dataloader=train_dataloader, val_dataloaders=val_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RUN_RNN:\n    trainer.save_checkpoint(\"sess_and_model.ckpt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def graph_expected_vs_preds(sample, out, filename=None):\n    sfreq = dataset.sfreq\n    NUM_CHANNELS = dataset.num_ch\n\n    t_start = 0.0\n    t_end = sample.shape[1] / sfreq # Full sample\n    times = np.arange(t_start, t_end, 1.0/sfreq)\n\n    # Extract data for t_start -> t_end seconds, for first NUM_CHANNELS channels\n    expected_eeg = sample[0, int(sfreq * t_start):int(sfreq * t_end), :NUM_CHANNELS]\n    predicted_eeg = out.detach().numpy()[0, int(sfreq * t_start):int(sfreq * t_end), :NUM_CHANNELS]\n\n\n    NUM_COLS = 2\n    NUM_ROWS = NUM_CHANNELS# math.ceil(NUM_CHANNELS / NUM_COLS)\n\n    fig, axs = plt.subplots(NUM_ROWS, NUM_COLS, figsize=(20, 50), sharex=True)\n    fig.tight_layout(pad=2.0)\n\n    chart_row = 0\n    chart_col = 0\n    for ch_num, ch_name in enumerate(dataset.ch_names[:NUM_CHANNELS]):\n#         axs[chart_row, chart_col].plot(times, expected_eeg[:, ch_num], label=\"Expected EEG\")\n        axs[chart_row, chart_col].plot(times, predicted_eeg[:, ch_num], label=\"Predicted EEG\")\n        axs[chart_row, chart_col].title.set_text(ch_name + \" - Predicted EEG\")\n\n        axs[chart_row, chart_col+1].plot(times, expected_eeg[:, ch_num]) # - predicted_eeg[:, ch_num])\n        axs[chart_row, chart_col+1].title.set_text(ch_name + \" - Expected EEG\")\n\n#         if chart_row == NUM_ROWS-1:\n#             handles, labels = axs[chart_row, chart_col].get_legend_handles_labels()\n#             fig.legend(handles, labels, loc='upper center')\n\n        chart_row += 1\n\n    plt.xlabel('Seconds')\n    plt.ylabel('$\\mu V$')\n    \n    if filename:\n        plt.savefig(filename, dpi=150)\n    else:\n        plt.show()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RUN_RNN:\n    model.eval() # To turn on training mode, model.train() - funcs from PyTorch nn.Module\n\n    sample = next(iter(val_dataloader))\n    sample = sample[0:1]\n\n    encoder_out, encoder_hidden, out, decoder_hidden = model(sample)\n    graph_expected_vs_preds(sample, out, filename=\"basic-rnn-validation-results.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RUN_RNN:\n    model.eval() # To turn on training mode, model.train() - funcs from PyTorch nn.Module\n    \n    line_height = 321\n    length = 10\n    sample = torch.ones((1, int(length*sfreq), NUM_CHANNELS)) + torch.unsqueeze(torch.linspace(0, line_height, int(length*sfreq))[:, None], 0)\n    sample = sample[0:1]\n\n    encoder_out, encoder_hidden, out, decoder_hidden = model(sample)\n    graph_expected_vs_preds(sample, out, filename=\"basic-rnn-assert-line-test.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RUN_RNN:\n    model.eval() # To turn on training mode, model.train() - funcs from PyTorch nn.Module\n\n    amplitude = 10\n    freq = 80\n    length = 10\n\n    sine_wave = torch.sin(torch.linspace(0, 2*np.pi*freq*length, int(length*sfreq)))\n    sample = torch.ones((1, int(length*sfreq), NUM_CHANNELS)) + torch.unsqueeze(sine_wave[:, None], 0) # next(iter(val_dataloader))\n    sample = sample[0:1]\n\n    encoder_out, encoder_hidden, out, decoder_hidden = model(sample)\n    graph_expected_vs_preds(sample, out, filename=\"basic-rnn-assert-80Hz-sine-test.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# VAE with Power Spectrum Density / Spectrograms","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\n\nclass PSDDataset(Dataset):\n    \n    def __init__(self, epochs_df, drop_channels=[], transform=None):\n        self.df = epochs_df\n        self.transform=transform\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        item = self.df.iloc[index]\n        x, target, pid = np.load(item['filename']), item['label'], item['patient_id']\n        if self.transform!=None:\n            x = self.transform(x)\n        return torch.tensor(x).float(), target, pid","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GaussianVAE(pl.LightningModule):\n    def __init__(self, input_shape, hidden=500, latent_dim=4):\n        super().__init__()\n        self.img_dim = np.prod(input_shape)\n        self.encoder = nn.Sequential(\n            nn.Linear(self.img_dim, hidden),\n            nn.Tanh(),\n            nn.Linear(hidden, latent_dim)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim//2, hidden),\n            nn.Tanh(),\n            nn.Linear(hidden, self.img_dim*2)\n        )\n        self.lr = 1.0e-3\n    \n    def unpack_var_params(self, params, d=2):\n        mu, logvar = params[:, :d], params[:, d:]\n        mu,logvar = mu.view(-1,d),logvar.view(-1, d)\n        return mu, logvar\n    \n    def unpack_decoder_params(self, params):\n        n = params.size(1)//2\n        mu, logvar = params[:, :n], params[:, n:]\n        return mu, logvar\n    \n    def sample(self, q_mu, q_logsigma):\n        z = torch.randn_like(q_mu, device=self.device)*torch.exp(q_logsigma) + q_mu \n        return z\n\n    def gaussian_log_loss(self, x, recon_x):\n        mu_x, logvar_x = self.unpack_decoder_params(recon_x)\n        part1 = torch.mean(logvar_x)\n        sigma = logvar_x.mul(0.5).exp_()\n        part2 = torch.mean(((x - mu_x) / sigma) ** 2)\n        gll = .5 * (part1 + part2)\n        return gll\n    \n    def KLD(self, mu, logvar):\n        kld = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n        return kld\n\n    def loss(self, x, x_rec, q_mu, q_logvar):\n        \"\"\" \n            returns: (batch_size, h*w)\n        \"\"\"\n        return self.gaussian_log_loss(x, x_rec) + self.KLD(q_mu, q_logvar)\n            \n    def forward(self, x):\n        var_params = self.encoder(x) # Variational params from data\n        q_mu, q_logsigma = self.unpack_var_params(var_params)\n        z = self.sample(q_mu, q_logsigma) # z = batch_size x 2\n        decode_out = self.decoder(z)\n        return decode_out, q_mu, q_logsigma\n        \n    def training_step(self, batch, batch_idx):\n        x, t, _ = batch\n        x = x.view(-1, self.img_dim)\n        x_rec, q_mu, q_logsigma = self(x)\n        loss = self.loss(x, x_rec, q_mu, q_logsigma)\n        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, t, _ = batch\n        x = x.view(-1, self.img_dim)\n        x_rec, q_mu, q_logsigma = self(x)\n        loss = self.loss(x, x_rec, q_mu, q_logsigma)\n        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer\n    \n    \nclass BernoulliVAE(pl.LightningModule):\n    def __init__(self, input_shape, hidden=500, latent_dim=4):\n        super().__init__()\n        self.img_dim = np.prod(input_shape)\n        self.encoder = nn.Sequential(\n            nn.Linear(self.img_dim, hidden),\n            nn.Tanh(),\n            nn.Linear(hidden, latent_dim)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim//2, hidden),\n            nn.Tanh(),\n            nn.Linear(hidden, self.img_dim)\n        )\n        self.lr = 1.0e-3\n        \n    def gaussian_logprob(self, x, mu=None, log_std=None, eps=1e-7):\n        if mu==None:\n            mu = torch.tensor([0], device=self.device).float()\n        if log_std==None:\n            log_std = mu\n        return -( 0.5*torch.log(2*torch.tensor([np.pi], device=self.device).float()) + log_std + 0.5*((x-mu)/(torch.exp(log_std)+eps))**2)\n    \n    def bernoulli_log_density(self, x, logit_means): # TODO: change this to gaussian\n        \"\"\" x: (batch_size, h*w)\n            logit_means: (batch_size, h*w)\n            returns: (batch_size, h*w)\n            p^x*(1-p)^(1-x) --> xlogp + (1-x)log(1-p)\n        \"\"\"\n        b = x*2-1 # [0,1] -> [-1,1]\n        return -torch.log1p(torch.exp(-b*logit_means))\n    \n    def log_prior(self, z):\n        \"\"\"z: (batch_size, 2)\n        returns: (batch_size, 1)\"\"\"\n        return 2*self.gaussian_logprob(z) # since 2D gaussian ~ N(0, 2I)\n\n    def joint_log_density(self, x, z, y):\n        \"\"\" x: (batch_size, h*w)\n            z: (batch_size, 2)\n            returns: (batch_size, 1)\n        \"\"\"\n        l_prior = self.log_prior(z)\n        ll = self.bernoulli_log_density(x, y)\n        return l_prior.sum(axis=1) + ll.sum(axis=1)\n\n    def log_q(self, z, q_mu, q_logsigma):\n        \"\"\" z: (batch_size, 2)\n            q_mu: (batch_size, 2)\n            q_logsigma: (batch_size, 2)\n            returns: (batch_size, 1)\n        \"\"\"\n        return self.gaussian_logprob(z, q_mu, q_logsigma).sum(axis=1)\n    \n    def sample(self, q_mu, q_logsigma):\n        z = torch.randn_like(q_mu, device=self.device)*torch.exp(q_logsigma) + q_mu \n        return z\n\n    def unpack_var_params(self, params, d=2):\n        mu, logvar = params[:, :d], params[:, d:]\n        mu,logvar = mu.view(-1,d),logvar.view(-1, d)\n        return mu, logvar\n    \n    def neg_elbo(self, joint_ll, log_q_z):\n        \"\"\" x: (batch_size, h*w)\n            returns: scalar\n        \"\"\"\n        elbo_estimate = torch.mean(joint_ll - log_q_z) # scalar, mean variational ELBO over batch\n        return -elbo_estimate\n    \n    def forward(self,x):\n        var_params = self.encoder(x) # Variational params from data\n        q_mu, q_logsigma = self.unpack_var_params(var_params)\n        z = self.sample(q_mu, q_logsigma) # z = batch_size x 2\n        y = self.decoder(z)\n        return y, z, q_mu, q_logsigma\n    \n    def training_step(self, batch, batch_idx):\n        x, t, _ = batch\n        x = x.view(-1, self.img_dim)\n        y, z, q_mu, q_logsigma = self(x)\n        joint_ll = self.joint_log_density(x, z, y) # joint likelihood of z and x under model -- batch_size x 1\n        log_q_z = self.log_q(z, q_mu, q_logsigma) # likelihood of z under variational distribution -- batch_size x 1\n        loss = self.neg_elbo(joint_ll, log_q_z)\n        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        x, t, _ = batch\n        x = x.view(-1, self.img_dim)\n        y, z, q_mu, q_logsigma = self(x)\n        joint_ll = self.joint_log_density(x, z, y) # joint likelihood of z and x under model -- batch_size x 1\n        log_q_z = self.log_q(z, q_mu, q_logsigma) # likelihood of z under variational distribution -- batch_size x 1\n        loss = self.neg_elbo(joint_ll, log_q_z)\n        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n        return optimizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RUN_VAE:\n    MU, SD = torch.load('/kaggle/working/means.pt'), torch.load('/kaggle/working/stds.pt')\n    dataset = PSDDataset(epochs,\n                         transform=torchvision.transforms.Compose([\n                                      torchvision.transforms.ToTensor(),\n                                      lambda x: (x-MU)/SD, # > 0, # binarize\n                                      lambda x: x.float()\n                                  ])\n                        )\n\n    train_len = int(len(dataset)*0.90)\n    val_len = len(dataset)-train_len\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, val_len])\n\n    train_dataloader = DataLoader(train_dataset,batch_size=64, shuffle=True)\n    val_dataloader = DataLoader(val_dataset,batch_size=64, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LOAD = False\ncheckpoint_path = \"/kaggle/working/sess_and_model_vae.ckpt\"\npsd_shape = np.array([24, 129])\nVAE = {'gaussian': GaussianVAE, 'ber': BernoulliVAE}\nll = 'gaussian'\nif RUN_VAE:\n    model = VAE.load_from_checkpoint(checkpoint_path=checkpoint_path, input_shape=psd_shape) if LOAD else VAE[ll](psd_shape)\n    trainer = pl.Trainer(gpus=1,\n                         logger=LiveLossLogger(),\n                         max_epochs=100,\n                         callbacks=[\n                             pl.callbacks.early_stopping.EarlyStopping(\n                                 monitor='train_loss',\n                                 mode='min',\n                                 patience=5\n                             )\n                         ],\n                        auto_lr_find=True)\n    trainer.fit(model, train_dataloader=train_dataloader, val_dataloaders=val_dataloader)\n    trainer.save_checkpoint(\"/kaggle/working/sess_and_model_vae.ckpt\")\n   # torch.save(model, \"/kaggle/working/vae.pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Latent Distribution of Batch","metadata":{}},{"cell_type":"markdown","source":"###  Training set","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n\ndef plot_latent_distribution(model, dataloader, legend=\"epilepsy\"):\n    zs=[]\n    labels=[]\n    pat=[]\n    for idx, (x, t, pid) in enumerate(dataloader):\n        x_rec, q_mu, q_ls = model(x.view(-1,model.img_dim))\n        q_mu = q_mu.detach().numpy()\n        zs.append(q_mu)\n        labels.append(t)\n        pat.append(pid)\n    zs = np.vstack(zs);print(zs.shape)\n    labels=np.concatenate(labels)\n    pat = np.concatenate(pat)\n    sns.set(rc={'figure.figsize':(10,8)})\n    if legend=='epilepsy':\n        colors = labels\n    else:\n        colors = pat\n    sns.scatterplot(x=zs[:,0], y=zs[:,1], hue=colors, palette='deep', legend='full')    \n    return zs, labels\n\nif RUN_VAE:\n    model.eval()\n    zs, labels = plot_latent_distribution(model, train_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RUN_VAE:\n    model.eval()\n    zs, labels = plot_latent_distribution(model, train_dataloader, legend=\"patients\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validation set","metadata":{}},{"cell_type":"code","source":"if RUN_VAE:\n    model.eval()\n    zs, labels = plot_latent_distribution(model, val_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RUN_VAE:\n    model.eval()\n    zs, labels = plot_latent_distribution(model, val_dataloader, legend=\"patients\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize Reconstructed PSDs with Trained VAE","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef transform_logit(y):\n    return torch.exp(y)/(1+torch.exp(y))\n\nif RUN_VAE:\n    model.eval() # To turn on training mode, model.train() - funcs from PyTorch nn.Module\n    h,w = psd_shape\n    N = 25\n    img = np.zeros((N*h, w*2))\n    img2 = np.zeros((N*h, w))\n    for idx, (sample, _, _) in enumerate(train_dataloader):\n        if idx==25:\n            break\n        x = sample[0]\n        x = x.view(-1, model.img_dim)\n        x_rec, _,_ = model(x)\n        #img[idx*h:(idx+1)*h, :w] = sample[0]\n        img[idx*h:(idx+1)*h, w:] = x_rec.view(-1, h, w).detach().numpy()[1] # > 0\n        img[idx*h:(idx+1)*h, :w] = sample[0]  #> 0\n    fig = plt.figure(figsize=(12,24))\n    plt.imshow(img,vmin=-1,vmax=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BernoulliVAE","metadata":{}},{"cell_type":"code","source":"if RUN_VAE:\n    MU, SD = torch.load('/kaggle/working/means.pt'), torch.load('/kaggle/working/stds.pt')\n    dataset = PSDDataset(epochs,\n                         transform=torchvision.transforms.Compose([\n                                      torchvision.transforms.ToTensor(),\n                                      lambda x: (x-MU)/SD > 0, # binarize\n                                      lambda x: x.float()\n                                  ])\n                        )\n\n    train_len = int(len(dataset)*0.90)\n    val_len = len(dataset)-train_len\n    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, val_len])\n\n    train_dataloader = DataLoader(train_dataset,batch_size=64, shuffle=True)\n    val_dataloader = DataLoader(val_dataset,batch_size=64, shuffle=False)\n\n    LOAD = False\n    checkpoint_path = \"/kaggle/working/sess_and_model_vae.ckpt\"\n    psd_shape = np.array([24, 129])\n    VAE = {'gaussian': GaussianVAE, 'ber': BernoulliVAE}\n    ll = 'ber'\n\n    model_ber = VAE.load_from_checkpoint(checkpoint_path=checkpoint_path, input_shape=psd_shape) if LOAD else VAE[ll](psd_shape)\n    trainer = pl.Trainer(gpus=1,\n                         logger=LiveLossLogger(),\n                         max_epochs=100,\n                         callbacks=[\n                             pl.callbacks.early_stopping.EarlyStopping(\n                                 monitor='train_loss',\n                                 mode='min',\n                                 patience=5\n                             )\n                         ],\n                        auto_lr_find=True)\n    trainer.fit(model_ber, train_dataloader=train_dataloader, val_dataloaders=val_dataloader)\n    trainer.save_checkpoint(\"/kaggle/working/sess_and_model_vae.ckpt\")\n   # torch.save(model_ber, \"/kaggle/working/vae.pt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training set Latent Distribution","metadata":{}},{"cell_type":"code","source":"def plot_latent_distribution(model_ber, dataloader, legend='epilepsy'):\n    zs=[]\n    labels=[]\n    pat=[]\n    for idx, (x, t, pid) in enumerate(dataloader):\n        x = x.view(-1, psd_shape[0]*psd_shape[1])\n        logit_means, z, q_mu, q_ls = model_ber(x)\n        q_mu = q_mu.detach().numpy()\n        zs.append(q_mu)\n        labels.append(t)\n        pat.append(pid)\n    zs = np.vstack(zs);print(zs.shape)\n    labels=np.concatenate(labels)\n    pat = np.concatenate(pat)\n    sns.set(rc={'figure.figsize':(10,8)})\n    if legend=='epilepsy':\n        colors = labels\n    else:\n        colors = pat\n    sns.scatterplot(x=zs[:,0], y=zs[:,1], hue=colors, palette='deep', legend='full')\n    return zs, labels\n\nif RUN_VAE:\n    model_ber.eval()\n    q_mu, labels = plot_latent_distribution(model_ber, train_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RUN_VAE:\n    model_ber.eval()\n    zs, labels = plot_latent_distribution(model_ber, train_dataloader, legend=\"patients\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validation set Latent Distribution","metadata":{}},{"cell_type":"code","source":"if RUN_VAE:\n    model_ber.eval()\n    q_mu, labels = plot_latent_distribution(model_ber, val_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RUN_VAE:\n    model_ber.eval()\n    zs, labels = plot_latent_distribution(model_ber, val_dataloader, legend=\"patients\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_logit(y):\n    return torch.exp(y)/(1+torch.exp(y))\n\nif RUN_VAE:\n    model_ber.eval() # To turn on training mode, model.train() - funcs from PyTorch nn.Module\n    h,w = psd_shape\n    N = 25\n    img = np.zeros((N*h, w*2))\n    img2 = np.zeros((N*h, w))\n    for idx, batch in enumerate(train_dataloader):\n        if idx==25:\n            break\n        sample, t, pid = batch\n        x = sample[0]\n        x = x.view(-1, model_ber.img_dim)\n        y,_,_,_ = model_ber(x)\n        mu = transform_logit(y)\n        #img[idx*h:(idx+1)*h, :w] = sample[0]\n        img[idx*h:(idx+1)*h, w:] = mu.view(-1, h, w).detach().numpy()\n        img[idx*h:(idx+1)*h, :w] = sample[0]\n    fig = plt.figure(figsize=(12,24))\n    plt.imshow(img)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Latent Interpolation Along Lattice","metadata":{}},{"cell_type":"code","source":"def plot_learned_latent_space(model, n=50, z0=(-6, 6), z1=(-6, 6)):\n    h, w = 24, 129\n    img = np.zeros((n*h, n*w))\n    for i, y in enumerate(np.linspace(*z1, n)):\n        for j, x in enumerate(np.linspace(*z0, n)):\n            z = torch.Tensor([[x, y]])\n            x_ber = transform_logit(model.decoder(z))\n            x_ber = x_ber.reshape(h, w).detach().numpy()\n            img[(n-1-i)*h:(n-1-i+1)*h, j*w:(j+1)*w] = x_ber\n    fig=plt.figure(figsize=(30,30))\n    plt.imshow(img, extent=[*z0, *z1])\n\nif RUN_VAE:\n    model_ber.eval()\n    plot_learned_latent_space(model_ber, n=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_ber","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}